{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Dataset imports\n",
    "from dataloaders.dynamic_edits import DynamicEdits\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transforms.jpegify import JpegCompress\n",
    "\n",
    "# Stuff for displaying progress\n",
    "import io\n",
    "import datetime\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Trainer\n",
    "import trainers.image_improvement\n",
    "import trainers.pix2pix\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tensor_to_pil(tensor):\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.squeeze()\n",
    "    return to_pil(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def image_to_byte_array(image:Image):\n",
    "    # Converts a PIL image to byteArray\n",
    "    imgByteArr = io.BytesIO()\n",
    "    image.save(imgByteArr, format=\"png\")\n",
    "    imgByteArr = imgByteArr.getvalue()\n",
    "    return imgByteArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_out_chain(renders, image_width, image_height):\n",
    "    image_count = len(renders)\n",
    "    out_chain = Image.new('RGB', (image_width, image_height))\n",
    "    widths = [x.width for x in renders]\n",
    "    for i,im in enumerate(renders):\n",
    "        w = 0 if i==0 else sum(widths[:i])\n",
    "        out_chain.paste(im, (w, 0))\n",
    "    out_chain = image_to_byte_array(out_chain)\n",
    "    return out_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 21 16:08:46 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.95.01    Driver Version: 440.95.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 105...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 41%   67C    P0    N/A /  75W |      1MiB /  4038MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fixJpg_percept'\n",
    "bsize = 4\n",
    "epoch = 0 #epoch to start\n",
    "n_epochs = 100 #Epoch to end\n",
    "html_at = 10\n",
    "save_epoch_freq = 10\n",
    "use_latest = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_t = transforms.Compose([transforms.RandomCrop(256, pad_if_needed=True)])\n",
    "target_t = transforms.Compose([JpegCompress(5,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DynamicEdits(root='./datasets/hires_fem', transform=source_t, target_transform=target_t)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=bsize, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_chain = create_out_chain(renders=[tensor_to_pil(x) for x in train_data[0]], image_width=256, image_height=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166c1fb5b02f492785f2fa91cb7d9f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x08\\x02\\x00\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_container = widgets.Image(\n",
    "    value=out_chain,\n",
    "    format='png',\n",
    "    width=256*3,\n",
    "    height=256,\n",
    ")\n",
    "\n",
    "log_out = widgets.Label(\n",
    "    value='hello',\n",
    ")\n",
    "w = widgets.VBox([image_container, log_out])\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = trainers.image_improvement.ImageImprovementTrainer(model_name=model_name, lambda_pixel=50, epoch=epoch, use_latest=use_latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b2673d014548df91dd1b94b2524cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vkumar/miniconda3/envs/ai/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/vkumar/miniconda3/envs/ai/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f54352e32527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhtml_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresulting_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mout_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34mf\"[{ep:05d}/{n_epochs:05d}][{batch_idx:05d}/{len(train_loader):05d}]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/gan/aiplayground/trainers/image_improvement.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Values to keep track of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m##########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         ret_loss = [('lossG', loss_g.mean().item()),\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;34m'pixel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_pixel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;34m'percept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_perception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "it = 1\n",
    "for ep in range(epoch,n_epochs):\n",
    "    for batch_idx, data in tqdm(enumerate(train_loader), total=html_at):\n",
    "        it += 1\n",
    "        losses, resulting_image = model.step(inputs=data)\n",
    "        out_txt = [ f\"[{ep:05d}/{n_epochs:05d}][{batch_idx:05d}/{len(train_loader):05d}]\"]\n",
    "        for k,v in losses:\n",
    "            out_txt.append(f\"{k}:{v:.4f}\")\n",
    "        message = \" \".join(out_txt)\n",
    "        log_out.value=message\n",
    "\n",
    "        if (it % html_at) == 0 or (ep==0 and batch_idx==0):\n",
    "            model.save('latest')\n",
    "            renders = [tensor_to_pil(x) for x in [data[0][0], resulting_image[0], data[1][0]]]\n",
    "            image_width = 256*3\n",
    "            image_height = 256\n",
    "            c = create_out_chain(renders, image_width, image_height)\n",
    "            image_container.value = c\n",
    "            image_container.width = image_width\n",
    "            image_container.height = image_height\n",
    "\n",
    "    if ep % save_epoch_freq == 0:\n",
    "        model.save('%d' % ep)\n",
    "        log_out.value=f\"saved model(s) {ep}\"\n",
    "\n",
    "    now=datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    s = f\"epoch {ep} completed. {now}\"\n",
    "    log_out.value = s\n",
    "    # End single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
